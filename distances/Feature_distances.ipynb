{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(2)\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_transforms = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            #transforms.CenterCrop(constants.INPUT_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 734\n",
      "    Root Location: imgs_by_label/celeba_unlabeled/\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=299, interpolation=PIL.Image.BILINEAR)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 66\n",
      "    Root Location: imgs_by_label/celeba_labeled/\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=299, interpolation=PIL.Image.BILINEAR)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2233\n",
      "    Root Location: imgs_by_label/progan_labeled/\n",
      "    Transforms (if any): Compose(\n",
      "                             Resize(size=299, interpolation=PIL.Image.BILINEAR)\n",
      "                             ToTensor()\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "unlabeled_celeba = datasets.ImageFolder('imgs_by_label/celeba_unlabeled/', inception_transforms)\n",
    "print(unlabeled_celeba)\n",
    "unlabeled_celeba_loader = torch.utils.data.DataLoader(\n",
    "        unlabeled_celeba, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "labeled_celeba = datasets.ImageFolder('imgs_by_label/celeba_labeled/', inception_transforms)\n",
    "print(labeled_celeba)\n",
    "labeled_celeba_loader = torch.utils.data.DataLoader(\n",
    "        labeled_celeba, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "labeled_progan = datasets.ImageFolder('imgs_by_label/progan_labeled/', inception_transforms)\n",
    "print(labeled_progan)\n",
    "labeled_progan_loader = torch.utils.data.DataLoader(\n",
    "        labeled_progan, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inception_features(img_iter, device=None):\n",
    "    inception_net = models.inception_v3(pretrained=True, transform_input=True)\n",
    "    \n",
    "    layers_to_grab = [inception_net.Conv2d_1a_3x3, inception_net.Conv2d_2b_3x3,\n",
    "                 inception_net.Conv2d_3b_1x1, inception_net.Mixed_5d, inception_net.Mixed_6e,\n",
    "                 inception_net.Mixed_7c, inception_net.fc]\n",
    "    \n",
    "    layer_features = [None for i in range(len(layers_to_grab))]\n",
    "    \n",
    "    \n",
    "    def hook_fn(self, inp, out, container, layer_index):\n",
    "        #print(layer_index, inp[0].shape, out.shape)\n",
    "\n",
    "        num_channels = out.shape[1]\n",
    "        if len(out.shape) > 2:\n",
    "            #Warning: this will break for batch sizes > 1\n",
    "            cur_features = out.squeeze().permute(1,2,0).reshape(-1, num_channels)\n",
    "        else:\n",
    "            cur_features = out\n",
    "\n",
    "        if container[layer_index] is None:\n",
    "            container[layer_index] = [cur_features]\n",
    "        else:\n",
    "            #container[layer_index] = torch.cat((container[layer_index], cur_features))\n",
    "            container[layer_index].append(cur_features)\n",
    "\n",
    "    def hook_fn_i(container, i):\n",
    "        return lambda self, inp, out: hook_fn(self, inp, out, container, i)\n",
    "\n",
    "    for i, layer in enumerate(layers_to_grab):\n",
    "        layer.register_forward_hook(hook_fn_i(layer_features, i))\n",
    "        \n",
    "    inception_net.eval()\n",
    "\n",
    "    for x,y in tqdm_notebook(img_iter):\n",
    "        #print(x.shape, y)\n",
    "        #plt.imshow((x).squeeze().permute(1, 2, 0))\n",
    "        #plt.show()\n",
    "        out = inception_net(x.to(device))\n",
    "        del(out)\n",
    "        #print(out.sum())\n",
    "        \n",
    "    return layer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7508cce1504fa5923ca85c9c60699f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=734), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#unlabeled_celeba_features = get_inception_features(unlabeled_celeba_loader)\n",
    "\n",
    "#flat_unlabeled_celeba_features = [torch.cat(lf, dim=0) for lf in unlabeled_celeba_features]\n",
    "\n",
    "#print([(len(lf), lf[0].shape) for lf in unlabeled_celeba_features])\n",
    "#print([lf.shape for lf in flat_unlabeled_celeba_features])\n",
    "\n",
    "#torch.save(flat_unlabeled_celeba_features, 'flat_unlabeled_celeba_features.pt')\n",
    "# The features from these 734 reference images are 8.9 gigs on disk, yikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: modify the get_inception_features function to have a \"don't flatten\" mode for these?\n",
    "#Or just feed them in one at a time so we don't care about the flattening. (So only mess with\n",
    "#it if we need batch size > 1)\n",
    "#celeba_features = get_inception_features(labeled_celeba_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#progan_features_50 = get_inception_features(labeled_progan_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (for single image): #layers x (H*W for that layer) x (C for that layer)\n",
    "# Reference set (for N comparison images): # layers x (N*H*W for that layer) x (C for that layer)\n",
    "def layerwise_nn_features(features, reference_set):\n",
    "    assert(len(features) == len(reference_set))\n",
    "    L = len(features)\n",
    "    mean_layer_closest_dists = torch.zeros(L)\n",
    "    \n",
    "    for l in range(L):\n",
    "        lf = features[l] #layer features\n",
    "        rlf = reference_set[l] #reference layer features\n",
    "        \n",
    "        #layer is HxWxC\n",
    "        #rlf[i] is NxC\n",
    "        H,W,C = lf.shape\n",
    "        N,C2 = rlf.shape\n",
    "        assert(C == C2)\n",
    "\n",
    "        x = lf.reshape(H*W, 1, C)\n",
    "        cur_refs = rlf.reshape(1, N, C)\n",
    "\n",
    "        diffs = x - cur_refs\n",
    "        assert(diffs.shape == (H*W, N, C))\n",
    "\n",
    "        sqr_dists = torch.sum(diffs**2, dim=2)\n",
    "        assert(sqr_dists.shape == (H*W, N))\n",
    "\n",
    "        min_sqr_dists = torch.min(sqr_dists, dim=1)\n",
    "        assert(min_dists.shape == (H*W))\n",
    "        \n",
    "        min_dists = torch.sqrt(min_sqr_dists)\n",
    "        assert(min_dists.shape == (H*W))\n",
    "        \n",
    "        mean_layer_closest_dists[l] = torch.mean(min_dists) \n",
    "    \n",
    "    return mean_layer_closest_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_celeba_x = []\n",
    "labeled_celeba_y = []\n",
    "# TODO: Pull % rated as real for each image!\n",
    "for x,y in tqdm(labeled_celeba_loader):\n",
    "    cur_features = layerwise_nn_features(x, unlabeled_celeba_features)\n",
    "    labeled_celeba_x.append(cur_features)\n",
    "    \n",
    "    # Now pull the % label\n",
    "    labeled_celeba_y.append(pct_real_votes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_progan_x = []\n",
    "labeled_progan_y = []\n",
    "# TODO: Pull % rated as real for each image!\n",
    "for x,y in tqdm(labeled_progan_loader):\n",
    "    cur_features = layerwise_nn_features(x, unlabeled_celeba_features)\n",
    "    labeled_progan_x.append(cur_features)\n",
    "    \n",
    "    # Now pull the % label\n",
    "    labeled_progan_y.append(pct_real_votes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Break the features/labels into train/val/test and train a logistic regression model;\n",
    "#see how well it does out of sample!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
