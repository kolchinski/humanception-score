{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import torch\n",
    "from torchvision import models, transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "_CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16130MB, multi_processor_count=80)\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(device.type)\n",
    "print(torch.cuda.get_device_properties(device)) #/ 1024 / 1024 /1024\n",
    "cpu_device = torch.device('cpu')\n",
    "print(cpu_device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_transforms = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            #transforms.CenterCrop(constants.INPUT_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 734\n",
      "    Root location: imgs_by_label/celeba_unlabeled/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=299, interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 66\n",
      "    Root location: imgs_by_label/celeba_labeled/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=299, interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 2233\n",
      "    Root location: imgs_by_label/progan_labeled/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=299, interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "unlabeled_celeba = datasets.ImageFolder('imgs_by_label/celeba_unlabeled/', inception_transforms)\n",
    "print(unlabeled_celeba)\n",
    "unlabeled_celeba_loader = torch.utils.data.DataLoader(\n",
    "        unlabeled_celeba, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "labeled_celeba = datasets.ImageFolder('imgs_by_label/celeba_labeled/', inception_transforms)\n",
    "print(labeled_celeba)\n",
    "labeled_celeba_loader = torch.utils.data.DataLoader(\n",
    "        labeled_celeba, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "labeled_progan = datasets.ImageFolder('imgs_by_label/progan_labeled/', inception_transforms)\n",
    "print(labeled_progan)\n",
    "labeled_progan_loader = torch.utils.data.DataLoader(\n",
    "        labeled_progan, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inception_features(img_iter, device=None):\n",
    "    inception_net = models.inception_v3(pretrained=True, transform_input=True)\n",
    "    \n",
    "    layers_to_grab = [inception_net.Conv2d_1a_3x3, inception_net.Conv2d_2b_3x3,\n",
    "                 inception_net.Conv2d_3b_1x1, inception_net.Mixed_5d, inception_net.Mixed_6e,\n",
    "                 inception_net.Mixed_7c, inception_net.fc]\n",
    "    \n",
    "    layer_features = [None for i in range(len(layers_to_grab))]\n",
    "    \n",
    "    \n",
    "    def hook_fn(self, inp, out, container, layer_index):\n",
    "        #print(layer_index, inp[0].shape, out.shape)\n",
    "\n",
    "        num_channels = out.shape[1]\n",
    "        if len(out.shape) > 2:\n",
    "            #Warning: this will break for batch sizes > 1\n",
    "            cur_features = out.squeeze().permute(1,2,0).reshape(-1, num_channels)\n",
    "        else:\n",
    "            cur_features = out\n",
    "\n",
    "        if container[layer_index] is None:\n",
    "            container[layer_index] = [cur_features]\n",
    "        else:\n",
    "            #container[layer_index] = torch.cat((container[layer_index], cur_features))\n",
    "            container[layer_index].append(cur_features)\n",
    "\n",
    "    def hook_fn_i(container, i):\n",
    "        return lambda self, inp, out: hook_fn(self, inp, out, container, i)\n",
    "\n",
    "    for i, layer in enumerate(layers_to_grab):\n",
    "        layer.register_forward_hook(hook_fn_i(layer_features, i))\n",
    "        \n",
    "    inception_net.eval()\n",
    "\n",
    "    for x,y in tqdm_notebook(img_iter):\n",
    "        #print(x.shape, y)\n",
    "        #plt.imshow((x).squeeze().permute(1, 2, 0))\n",
    "        #plt.show()\n",
    "        out = inception_net(x.to(device))\n",
    "        del(out)\n",
    "        #print(out.sum())\n",
    "        \n",
    "    return layer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#unlabeled_celeba_features = get_inception_features(unlabeled_celeba_loader)\n",
    "\n",
    "#flat_unlabeled_celeba_features = [torch.cat(lf, dim=0) for lf in unlabeled_celeba_features]\n",
    "\n",
    "#print([(len(lf), lf[0].shape) for lf in unlabeled_celeba_features])\n",
    "#print([lf.shape for lf in flat_unlabeled_celeba_features])\n",
    "\n",
    "#torch.save(unlabeled_celeba_features, 'unlabeled_celeba_features.pt')\n",
    "\n",
    "torch.save(flat_unlabeled_celeba_features, 'flat_unlabeled_celeba_features.pt')\n",
    "# The features from these 734 reference images are 8.9 gigs on disk, yikes!\n",
    "\n",
    "del unlabeled_celeba_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_unlabeled_celeba_features = torch.load('flat_unlabeled_celeba_features.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([16295534, 32]),\n",
       " torch.Size([15861006, 64]),\n",
       " torch.Size([3911486, 80]),\n",
       " torch.Size([899150, 288]),\n",
       " torch.Size([212126, 768]),\n",
       " torch.Size([46976, 2048]),\n",
       " torch.Size([734, 1000])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer_feats.shape for layer_feats in flat_unlabeled_celeba_features]\n",
    "#flat_unlabeled_celeba_features[6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 32])\n",
      "torch.Size([10000, 64])\n",
      "torch.Size([10000, 80])\n",
      "torch.Size([10000, 288])\n",
      "torch.Size([10000, 768])\n",
      "torch.Size([10000, 2048])\n",
      "torch.Size([734, 1000])\n"
     ]
    }
   ],
   "source": [
    "#l = len(flat_unlabeled_celeba_features[0])\n",
    "#indices = torch.LongTensor(np.random.choice(range(l), size=100, replace=False))\n",
    "#small_ref_features = torch.index_select(flat_unlabeled_celeba_features[0], dim=0, index = indices)\n",
    "#print(small_ref_features.shape)\n",
    "\n",
    "\n",
    "small_celeba_features = []\n",
    "for layer_feats in flat_unlabeled_celeba_features:\n",
    "    l = len(layer_feats)\n",
    "    indices = torch.LongTensor(np.random.choice(range(l), size=min(l, 10000), replace=False))\n",
    "    small_layer_feats = torch.index_select(layer_feats, dim=0, index = indices).detach()\n",
    "    small_celeba_features.append(small_layer_feats)\n",
    "\n",
    "for small_layer in small_celeba_features:\n",
    "    print(small_layer.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(small_celeba_features, 'small_celeba_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32])\n",
      "tensor([0.2806, 0.0000, 1.3214, 3.8172, 0.4523, 3.9398, 0.0000, 1.0819, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.5223, 1.9543, 0.0000, 0.0000, 4.0598, 0.0149,\n",
      "        0.0000, 1.8791, 1.1010, 0.0000, 0.0000, 0.0000, 0.0000, 0.7962, 0.0000,\n",
      "        1.9226, 0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<SelectBackward>) \n",
      " tensor([0.2806, 0.0000, 1.3214, 3.8172, 0.4523, 3.9398, 0.0000, 1.0819, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.5223, 1.9543, 0.0000, 0.0000, 4.0598, 0.0149,\n",
      "        0.0000, 1.8791, 1.1010, 0.0000, 0.0000, 0.0000, 0.0000, 0.7962, 0.0000,\n",
      "        1.9226, 0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# small_ref_features = torch.index_select(flat_unlabeled_celeba_features[0], dim=0, index=torch.LongTensor([0,3,5]))\n",
    "# print(small_ref_features.shape)\n",
    "# print(small_ref_features[2], '\\n', flat_unlabeled_celeba_features[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: modify the get_inception_features function to have a \"don't flatten\" mode for these?\n",
    "#Or just feed them in one at a time so we don't care about the flattening. (So only mess with\n",
    "#it if we need batch size > 1)\n",
    "#celeba_features = get_inception_features(labeled_celeba_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0ca55481f74f1899583e64cff6aec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#progan_features_1000_2234 = get_inception_features(islice(labeled_progan_loader,1000,2234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "progan_features_1000_2234 = torch.load('progan_features_1000_2234.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(progan_features_1000_2234, 'progan_features_1000_2234.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del(progan_features_1000_2234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-1f8e89a92cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprogan_features_0_1000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progan_features_0_1000.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "progan_features_0_1000 = torch.load('progan_features_0_1000.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0689feef0b984adc94e02109a96c05bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = len(progan_features_0_1000)\n",
    "N = len(progan_features_0_1000[0])\n",
    "\n",
    "#N = 3\n",
    "\n",
    "small_progan_features_0_1000 = []\n",
    "for i in tqdm_notebook(range(N)):\n",
    "    example_features = []\n",
    "    for l in range(L):\n",
    "        cur_feats = progan_features_0_1000[l][i]\n",
    "        D = len(cur_feats)\n",
    "        #print(cur_feats.shape)\n",
    "        indices = torch.LongTensor(np.random.choice(range(D), size=min(D, 10000), replace=False))\n",
    "        small_cur_feats = torch.index_select(cur_feats, dim=0, index = indices).detach()\n",
    "        example_features.append(small_cur_feats)\n",
    "    small_progan_features_0_1000.append(example_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[torch.Size([10000, 32]), torch.Size([10000, 64]), torch.Size([5329, 80]), torch.Size([1225, 288]), torch.Size([289, 768]), torch.Size([64, 2048]), torch.Size([1, 1000])]\n"
     ]
    }
   ],
   "source": [
    "print(len(small_progan_features_0_1000))\n",
    "print([x.shape for x in small_progan_features_0_1000[999]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(small_progan_features_0_1000, 'small_progan_features_0_1000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a880eb10314742afd00a2c6f1ef401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1233), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = len(progan_features_1000_2234)\n",
    "N = len(progan_features_1000_2234[0])\n",
    "\n",
    "small_progan_features_1000_2234 = []\n",
    "for i in tqdm_notebook(range(N)):\n",
    "    example_features = []\n",
    "    for l in range(L):\n",
    "        cur_feats = progan_features_1000_2234[l][i]\n",
    "        D = len(cur_feats)\n",
    "        #print(cur_feats.shape)\n",
    "        indices = torch.LongTensor(np.random.choice(range(D), size=min(D, 10000), replace=False))\n",
    "        small_cur_feats = torch.index_select(cur_feats, dim=0, index = indices).detach()\n",
    "        example_features.append(small_cur_feats)\n",
    "    small_progan_features_1000_2234.append(example_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1233\n",
      "[torch.Size([10000, 32]), torch.Size([10000, 64]), torch.Size([5329, 80]), torch.Size([1225, 288]), torch.Size([289, 768]), torch.Size([64, 2048]), torch.Size([1, 1000])]\n"
     ]
    }
   ],
   "source": [
    "print(len(small_progan_features_1000_2234))\n",
    "print([x.shape for x in small_progan_features_1000_2234[1232]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(small_progan_features_1000_2234, 'small_progan_features_1000_2234.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_progan_features = small_progan_features_0_1000 + small_progan_features_1000_2234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(small_progan_features, 'small_progan_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1000, torch.Size([22201, 32])),\n",
       " (1000, torch.Size([21609, 64])),\n",
       " (1000, torch.Size([5329, 80])),\n",
       " (1000, torch.Size([1225, 288])),\n",
       " (1000, torch.Size([289, 768])),\n",
       " (1000, torch.Size([64, 2048])),\n",
       " (1000, torch.Size([1, 1000]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(len(lf), lf[0].shape) for lf in progan_features_0_1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([22201, 32]),\n",
       " torch.Size([21609, 64]),\n",
       " torch.Size([5329, 80]),\n",
       " torch.Size([1225, 288]),\n",
       " torch.Size([289, 768]),\n",
       " torch.Size([64, 2048]),\n",
       " torch.Size([1, 1000])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progan_0_feats = [lf[0] for lf in progan_features_0_1000]\n",
    "[lf.shape for lf in progan_0_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Parallelize as much as possible within memory\n",
    "#TODO: Run on GPU, see if it's faster\n",
    "\n",
    "# Features (for single image): #layers x (H*W for that layer) x (C for that layer)\n",
    "# Reference set (for N comparison images): # layers x (N*H*W for that layer) x (C for that layer)\n",
    "def layerwise_nn_features(features, reference_set, device, batch_size=1):\n",
    "    \n",
    "    assert(len(features) == len(reference_set))\n",
    "    L = len(features)\n",
    "    #print(L)\n",
    "    mean_layer_closest_dists = torch.zeros(L).to(device)\n",
    "    \n",
    "    for l in range(L):\n",
    "        #print(l)\n",
    "        lf = features[l].detach().to(device) #layer features\n",
    "        rlf = reference_set[l].detach().to(device) #reference layer features\n",
    "\n",
    "        #print(lf.shape, rlf.shape)\n",
    "        \n",
    "        \n",
    "        #layer is HxWxC\n",
    "        #rlf[i] is NxC\n",
    "        HtimesW,C = lf.shape\n",
    "        N,C2 = rlf.shape\n",
    "        assert(C == C2)\n",
    "        \n",
    "        rlf = rlf.reshape(1, N, C).detach()\n",
    "        \n",
    "        \n",
    "        num_batches = HtimesW // batch_size\n",
    "        if HtimesW % batch_size != 0: num_batches += 1 # for the fractional batch\n",
    "            \n",
    "        #Loop through each feature vector, we can parallelize later...\n",
    "        #Note: if batch size does not divide HtimesW, this will miss the last HtimesW%batch_size vectors\n",
    "        for b in range(num_batches):\n",
    "            x = lf[b*batch_size : (b+1) * batch_size].reshape(-1, 1, C)\n",
    "            cur_batch_size = x.shape[0]\n",
    "            \n",
    "            #Differences from vector to all reference vectors in that layer\n",
    "            diffs = (x - rlf).detach()\n",
    "            assert(diffs.shape == (cur_batch_size, N, C))\n",
    "            \n",
    "            sqr_dists = torch.sum(diffs**2, dim=2).detach()\n",
    "            assert(sqr_dists.shape == (cur_batch_size, N))\n",
    "            \n",
    "            min_sqr_dists = torch.min(sqr_dists, dim=1)[0].detach()\n",
    "            assert(min_sqr_dists.shape == (cur_batch_size,))\n",
    "            \n",
    "            min_dists = torch.sqrt(min_sqr_dists).detach()\n",
    "            assert(min_dists.shape == (cur_batch_size,))\n",
    "            \n",
    "            mean_layer_closest_dists[l] += torch.sum(min_dists).detach()\n",
    "            \n",
    "            del x\n",
    "            del diffs\n",
    "            del sqr_dists\n",
    "            del min_sqr_dists\n",
    "            del min_dists\n",
    "            \n",
    "        mean_layer_closest_dists[l] /= (HtimesW)\n",
    "        \n",
    "        del lf\n",
    "        del rlf\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        continue\n",
    "\n",
    "#         x = lf.reshape(HtimesW, 1, C)\n",
    "#         cur_refs = rlf.reshape(1, N, C)\n",
    "\n",
    "#         diffs = x - cur_refs\n",
    "#         assert(diffs.shape == (H*W, N, C))\n",
    "\n",
    "#         sqr_dists = torch.sum(diffs**2, dim=2)\n",
    "#         assert(sqr_dists.shape == (H*W, N))\n",
    "\n",
    "#         min_sqr_dists = torch.min(sqr_dists, dim=1)\n",
    "#         assert(min_dists.shape == (H*W))\n",
    "        \n",
    "#         min_dists = torch.sqrt(min_sqr_dists)\n",
    "#         assert(min_dists.shape == (H*W))\n",
    "        \n",
    "#         mean_layer_closest_dists[l] = torch.mean(min_dists) \n",
    "    \n",
    "    return mean_layer_closest_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "#torch.zeros(4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1 = progan_0_feats[-1]\n",
    "#x2 = small_celeba_features[-1]\n",
    "\n",
    "#torch.min(torch.sqrt(((x1 - x2)**2).sum(dim=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2233, 7)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_progan_features), len(small_progan_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2233 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1c2d029ae148a099cd3639d7e87169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2233), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N, L = len(small_progan_features), len(small_progan_features[0])\n",
    "print(N,L)\n",
    "small_progan_distance_features = []\n",
    "for progan_i_feats in tqdm_notebook(small_progan_features):\n",
    "    distance_features = layerwise_nn_features(progan_i_feats, small_celeba_features, device, 32)\n",
    "    small_progan_distance_features.append(distance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2233, 7),\n",
       " array([ 0.03934574,  0.25115722,  0.34143984,  4.5206304 ,  3.1757584 ,\n",
       "        17.59048   , 16.474287  ], dtype=float32))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(small_progan_distance_features), small_progan_distance_features[0]\n",
    "progan_features_full = np.array(([np.array(x.cpu().detach()) for x in small_progan_distance_features]))\n",
    "progan_features_full.shape, progan_features_full[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(progan_features_full, 'progan_features_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 0.0395,  0.2486,  0.3414,  4.5206,  3.1758, 17.5905, 16.4743],\n",
       "        device='cuda:3'),\n",
       " tensor([ 0.0263,  0.1677,  0.2366,  4.0057,  2.6711, 12.5356, 10.4338],\n",
       "        device='cuda:3'),\n",
       " tensor([ 0.0271,  0.1819,  0.2637,  4.1718,  3.0865, 17.0902, 15.1371],\n",
       "        device='cuda:3'),\n",
       " tensor([ 0.0244,  0.1321,  0.1840,  3.6864,  3.3735, 16.2496, 14.1842],\n",
       "        device='cuda:3')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progan_1000_distance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "0\n",
      "torch.Size([22201, 32]) torch.Size([10000, 32])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737aa11cae93485787444de84baa28b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22201), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "torch.Size([21609, 64]) torch.Size([10000, 64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f18c9b0222404fbbb96647f18d3e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21609), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "torch.Size([5329, 80]) torch.Size([10000, 80])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d24a6027eb4e3ab3c38b43d42b3a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5329), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3\n",
      "torch.Size([1225, 288]) torch.Size([10000, 288])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b57718d78e49d88a695be0b014f59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1225), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4\n",
      "torch.Size([289, 768]) torch.Size([10000, 768])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46061b9787bf488080dbb52cc0597b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=289), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5\n",
      "torch.Size([64, 2048]) torch.Size([10000, 2048])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1db94dc6b1e4014a743f25ba0f78a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6\n",
      "torch.Size([1, 1000]) torch.Size([734, 1000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044faf3791d94d828352d28c66299305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0395,  0.2486,  0.3414,  4.5206,  3.1758, 17.5905, 16.4743],\n",
       "       device='cuda:3')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layerwise_nn_features(progan_0_feats, small_celeba_features, device, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22201, 32])\n",
      "torch.Size([7])\n",
      "torch.Size([2, 16295534, 32])\n",
      "torch.Size([2, 1, 32])\n",
      "torch.Size([7])\n",
      "torch.Size([22201, 32])\n",
      "torch.Size([1, 16295534, 32])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "counts = defaultdict(int)\n",
    "\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            #print(type(obj), obj.size(), obj.device)\n",
    "            if obj.device == torch.device(3): \n",
    "                print(obj.shape)\n",
    "                del obj\n",
    "            counts[obj.size()] += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#for k,v in counts.items():\n",
    "#    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7]) 2\n",
      "torch.Size([1, 16295534, 32]) 4\n",
      "torch.Size([1]) 2\n",
      "torch.Size([1, 1, 32]) 2\n",
      "torch.Size([16295534, 32]) 1\n",
      "torch.Size([15861006, 64]) 1\n",
      "torch.Size([3911486, 80]) 1\n",
      "torch.Size([899150, 288]) 1\n",
      "torch.Size([212126, 768]) 1\n",
      "torch.Size([46976, 2048]) 1\n",
      "torch.Size([734, 1000]) 1\n",
      "torch.Size([22201, 32]) 1000\n",
      "torch.Size([21609, 64]) 1000\n",
      "torch.Size([5329, 80]) 1000\n",
      "torch.Size([1225, 288]) 1000\n",
      "torch.Size([289, 768]) 1000\n",
      "torch.Size([64, 2048]) 1000\n",
      "torch.Size([1, 1000]) 1000\n"
     ]
    }
   ],
   "source": [
    "counts = defaultdict(int)\n",
    "\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            #print(type(obj), obj.size())\n",
    "            counts[obj.size()] += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for k,v in counts.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_celeba_x = []\n",
    "labeled_celeba_y = []\n",
    "# TODO: Pull % rated as real for each image!\n",
    "for x,y in tqdm(labeled_celeba_loader):\n",
    "    cur_features = layerwise_nn_features(x, unlabeled_celeba_features)\n",
    "    labeled_celeba_x.append(cur_features)\n",
    "    \n",
    "    # Now pull the % label\n",
    "    labeled_celeba_y.append(pct_real_votes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_progan_x = []\n",
    "labeled_progan_y = []\n",
    "# TODO: Pull % rated as real for each image!\n",
    "for x,y in tqdm(labeled_progan_loader):\n",
    "    cur_features = layerwise_nn_features(x, unlabeled_celeba_features)\n",
    "    labeled_progan_x.append(cur_features)\n",
    "    \n",
    "    # Now pull the % label\n",
    "    labeled_progan_y.append(pct_real_votes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Break the features/labels into train/val/test and train a logistic regression model;\n",
    "#see how well it does out of sample!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
